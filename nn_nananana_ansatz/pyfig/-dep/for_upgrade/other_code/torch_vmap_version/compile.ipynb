{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import os\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","%load_ext autoreload \n","%autoreload 2\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'lumi' requires ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n lumi ipykernel --update-deps --force-reinstall'"]}],"source":["# TORCH MNIST DISTRIBUTED EXAMPLE\n","\n","\"\"\"run.py:\"\"\"\n","#!/usr/bin/env python\n","import os\n","import torch\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","from pyfig import Pyfig\n","import numpy as np\n","\n","n_device = torch.cuda.device_count()\n","print(n_device)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('cuda: ', device, 'n_dev: ', n_device, 'device', device, 'name: ')\n","\n","torch.set_num_threads(1)\n","torch.manual_seed(1234)\n","torch.set_default_tensor_type(torch.DoubleTensor)   # ❗ Ensure works when default not set AND can go float32 or 64\n","\n","arg = dict(\n","    charge = 0,\n","    spin  = 0,\n","    a = np.array([[0.0, 0.0, 0.0],]),\n","    a_z  = np.array([4.,]),\n","    n_b = 256, \n","    n_sv = 32, \n","    n_pv = 16, \n","    n_corr = 40, \n","    n_step = 10000, \n","    log_metric_step = 5, \n","    exp_name = 'demo',\n","    # sweep = {},\n",")\n","\n","c = Pyfig(wb_mode='disabled', arg=arg, submit=False, run_sweep=False)\n","\n","n_device = c.n_device\n","print(f'🤖 {n_device} GPUs available')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","def forward(self, x_1):\n","    sin = torch.ops.aten.sin.default(x_1)\n","    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n","    ones_like = torch.ops.aten.ones_like.default(sum_1, dtype = torch.float64, layout = torch.strided, device = device(type='cpu'), pin_memory = False, memory_format = torch.preserve_format);  sum_1 = None\n","    expand = torch.ops.aten.expand.default(ones_like, [100]);  ones_like = None\n","    cos = torch.ops.aten.cos.default(x_1);  x_1 = None\n","    mul = torch.ops.aten.mul.Tensor(expand, cos);  expand = cos = None\n","    return mul\n","    \n"]}],"source":["from functorch import make_fx, grad\n","def f(x):\n","    return torch.sin(x).sum()\n","x = torch.randn(100)\n","grad_f = make_fx(grad(f))(x)\n","print(grad_f.code)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Run: ['git', 'log', '--pretty=format:%h', '-n', '1'] at /home/energy/amawi/projects/hwat\n","stdout: 3c51932 stderr: \n","Run: ['hostname'] at .\n","stdout: slid.fysik.dtu.dk  stderr: \n","torch.Size([256, 4, 3])\n"]},{"name":"stderr","output_type":"stream","text":["Resource temporarily unavailable (src/thread.cpp:269)\n"]},{"ename":"BlockingIOError","evalue":"[Errno 11] Resource temporarily unavailable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m r\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m lp \u001b[39m=\u001b[39m model(r[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m mod_com \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcompile(model)\n\u001b[1;32m     38\u001b[0m lp_com \u001b[39m=\u001b[39m mod_com(r[\u001b[39m0\u001b[39m])\n\u001b[1;32m     40\u001b[0m mod_script \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript(model)\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/__init__.py:1248\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, passes, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1247\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minductor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1248\u001b[0m     backend \u001b[39m=\u001b[39m _TorchCompileInductorWrapper(mode, passes)\n\u001b[1;32m   1249\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39moptimize(backend\u001b[39m=\u001b[39mbackend, nopython\u001b[39m=\u001b[39mfullgraph, dynamic\u001b[39m=\u001b[39mdynamic, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)(model)\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/__init__.py:1184\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__init__\u001b[0;34m(self, mode, passes)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meval_frame\u001b[39;00m \u001b[39mimport\u001b[39;00m lookup_backend\n\u001b[1;32m   1182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m InductorConfigContext\n\u001b[0;32m-> 1184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile_fn \u001b[39m=\u001b[39m lookup_backend(\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcm \u001b[39m=\u001b[39m InductorConfigContext(mode \u001b[39mif\u001b[39;00m mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m passes)\n\u001b[1;32m   1186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_torchdynamo_orig_callable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile_fn\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:377\u001b[0m, in \u001b[0;36mlookup_backend\u001b[0;34m(compiler_fn)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    369\u001b[0m             torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmatmul\u001b[39m.\u001b[39mallow_tf32 \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_capability() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    371\u001b[0m         ):\n\u001b[1;32m    372\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTensorFloat32 tensor cores for float32 matrix multiplication available but not enabled.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mConsider setting `torch.set_float32_matmul_precision(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m             )\n\u001b[0;32m--> 377\u001b[0m     compiler_fn \u001b[39m=\u001b[39m import_module(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mconfig\u001b[39m.\u001b[39;49minductor_import\u001b[39m}\u001b[39;49;00m\u001b[39m.compile_fx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mcompile_fx\n\u001b[1;32m    378\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(compiler_fn, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    379\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39moptimizations\u001b[39;00m \u001b[39mimport\u001b[39;00m BACKENDS\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug\u001b[39;00m \u001b[39mimport\u001b[39;00m DebugContext\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m select_decomp_table\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphLowering\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     dynamo_logging,\n\u001b[1;32m     22\u001b[0m     dynamo_optimizations,\n\u001b[1;32m     23\u001b[0m     dynamo_utils,\n\u001b[1;32m     24\u001b[0m     has_incompatible_cudagraph_ops,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvirtualized\u001b[39;00m \u001b[39mimport\u001b[39;00m V\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_inductor/graph.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m dynamo_config\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config, ir\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcodegen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m CppWrapperCodeGen, WrapperCodeGen\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     LoweringException,\n\u001b[1;32m     23\u001b[0m     MissingOperatorWithDecomp,\n\u001b[1;32m     24\u001b[0m     MissingOperatorWithoutDecomp,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mir\u001b[39;00m \u001b[39mimport\u001b[39;00m Constant, FixedLayout, InputBuffer, Pointwise, Reduction, TensorBox\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_inductor/codegen/wrapper.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m count\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, List\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m codecache, config, ir\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcodecache\u001b[39;00m \u001b[39mimport\u001b[39;00m cpp_compile_command, get_code_path\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m cache_on_self, dynamo_utils, has_triton, sympy_dot, sympy_product\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_inductor/codecache.py:669\u001b[0m\n\u001b[1;32m    664\u001b[0m                     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m    666\u001b[0m         _compile_end()\n\u001b[0;32m--> 669\u001b[0m AsyncCompile\u001b[39m.\u001b[39;49mwarm_pool()\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/site-packages/torch/_inductor/codecache.py:608\u001b[0m, in \u001b[0;36mAsyncCompile.warm_pool\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mcompile_threads):\n\u001b[0;32m--> 608\u001b[0m         pool\u001b[39m.\u001b[39;49m_adjust_process_count()\n\u001b[1;32m    609\u001b[0m     pool\u001b[39m.\u001b[39m_start_executor_manager_thread()\n\u001b[1;32m    610\u001b[0m _compile_end()\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/concurrent/futures/process.py:684\u001b[0m, in \u001b[0;36mProcessPoolExecutor._adjust_process_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m process_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_processes)\n\u001b[1;32m    677\u001b[0m \u001b[39mif\u001b[39;00m process_count \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_workers:\n\u001b[1;32m    678\u001b[0m     \u001b[39m# Assertion disabled as this codepath is also used to replace a\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39m# worker that unexpectedly dies, even when using the 'fork' start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[39m# we know a thread is running (self._executor_manager_thread).\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     \u001b[39m#assert self._safe_to_dynamically_spawn_children or not self._executor_manager_thread, 'https://github.com/python/cpython/issues/90622'\u001b[39;00m\n\u001b[0;32m--> 684\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawn_process()\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/concurrent/futures/process.py:701\u001b[0m, in \u001b[0;36mProcessPoolExecutor._spawn_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_spawn_process\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    695\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mp_context\u001b[39m.\u001b[39mProcess(\n\u001b[1;32m    696\u001b[0m         target\u001b[39m=\u001b[39m_process_worker,\n\u001b[1;32m    697\u001b[0m         args\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_queue,\n\u001b[1;32m    698\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result_queue,\n\u001b[1;32m    699\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initializer,\n\u001b[1;32m    700\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initargs))\n\u001b[0;32m--> 701\u001b[0m     p\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_processes[p\u001b[39m.\u001b[39mpid] \u001b[39m=\u001b[39m p\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/multiprocessing/context.py:277\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    276\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_fork\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n","File \u001b[0;32m~/miniconda3/envs/lumi/lib/python3.9/multiprocessing/popen_fork.py:66\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m parent_r, child_w \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpipe()\n\u001b[1;32m     65\u001b[0m child_r, parent_w \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpipe()\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfork()\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","\u001b[0;31mBlockingIOError\u001b[0m: [Errno 11] Resource temporarily unavailable"]},{"name":"stderr","output_type":"stream","text":["Process ForkProcess-1:\n","Traceback (most recent call last):\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n","    self.run()\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n","    call_item = call_queue.get(block=True)\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/queues.py\", line 103, in get\n","    res = self._recv_bytes()\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n","    buf = self._recv_bytes(maxlength)\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/home/energy/amawi/miniconda3/envs/lumi/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n"]}],"source":["\n","### model (aka Trainmodel) ### \n","from hwat_func import Ansatz_fb\n","from torch import nn\n","from hwat_func import compute_ke_b, compute_pe_b\n","from hwat_func import init_r, get_center_points\n","import wandb\n","from hwat_func import keep_around_points, sample_b\n","from utils import compute_metrix\n","\n","from functorch import vmap, make_functional, grad, make_fx\n","\n","_dummy = torch.randn((1,))\n","dtype = _dummy.dtype\n","c._convert(device=device, dtype=dtype)\n","model = c.partial(Ansatz_fb).to(device).to(dtype)\n","\n","center_points = get_center_points(c.data.n_e, c.app.a)\n","r = init_r(c.data.n_b, c.data.n_e, center_points, std=0.1)\n","deltar = torch.tensor([0.02]).to(device).to(dtype)\n","\n","# from torch.jit import Final    - https://pytorch.org/docs/stable/jit.html type safety and optimisations\n","\n","# from torch.profiler import profile, record_function, ProfilerActivity\n","\n","# with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n","#     with record_function(\"model_inference\"):\n","#         model(r[0])\n","\n","# # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n","# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n","print(r.shape)\n","\n","r.requires_grad_(False)\n","\n","lp = model(r[0])\n","\n","mod_com = torch.compile(model)\n","lp_com = mod_com(r[0])\n","\n","mod_script = torch.jit.script(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["FunctionalModule(\n","  (stateless_model): RecursiveScriptModule(\n","    original_name=Ansatz_fb\n","    (Vs): RecursiveScriptModule(\n","      original_name=ModuleList\n","      (0): RecursiveScriptModule(original_name=Linear)\n","      (1): RecursiveScriptModule(original_name=Linear)\n","    )\n","    (Ws): RecursiveScriptModule(\n","      original_name=ModuleList\n","      (0): RecursiveScriptModule(original_name=Linear)\n","      (1): RecursiveScriptModule(original_name=Linear)\n","    )\n","    (V_half_u): RecursiveScriptModule(original_name=Linear)\n","    (V_half_d): RecursiveScriptModule(original_name=Linear)\n","    (wu): RecursiveScriptModule(original_name=Linear)\n","    (wd): RecursiveScriptModule(original_name=Linear)\n","  )\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","### fix sampler\n","### fix train step \n","### metrix conversion\n","\n","model_fn, params = make_functional(model)\n","# model_v = torch.compile(model_fn)\n","model_v = torch.jit.script(model_fn)\n","model_v = vmap(model_fn, in_dims=(None, 0))\n","\n","model_v(params, r)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["opt = torch.optim.RAdam(model.parameters(), lr=0.0001)\n","\n","\n","wandb.define_metric(\"*\", step_metric=\"tr/step\")\n","for step in range(1, c.n_step+1):\n","    \n","    r, acc, deltar = sample_b(model_v, params, r, deltar, n_corr=c.data.n_corr)  # ❗needs testing \n","    r = keep_around_points(r, center_points, l=2.) if step < 1000 else r\n","    \n","    model_ke = lambda _r: model_v(params, _r).sum()\n","\n","    with torch.no_grad():\n","        ke = compute_ke_b(model_ke, r)\n","        pe = compute_pe_b(r, c.app.a, c.app.a_z)\n","        e = pe + ke\n","        e_mean_dist = torch.mean(torch.abs(torch.median(e) - e))\n","        e_clip = torch.clip(e, min=e-5*e_mean_dist, max=e+5*e_mean_dist)\n","\n","    # opt.zero_grad()\n","    loss_fn = lambda _params: ((e_clip - e_clip.mean())*model_v(_params, r)).mean()\n","    \n","    grads = grad(loss_fn)(params)\n","    for p, g in zip(model.parameters(), grads):\n","        p.grad = torch.nn.utils.clip_grad_norm_(g.clone(), max_norm=1.)\n","\n","    opt.step()\n","    \n","    params = [p.detach() for p in model.parameters()]\n","    grads = [p.grad.detach() for p in model.parameters()]\n","    \n","    v_tr = dict(\n","        params=params, grads=grads,\n","        e=e, pe=pe, ke=ke, r=r,\n","    )\n","    \n","    if not (step % c.log_metric_step):\n","        metrix = compute_metrix(v_tr)  # ❗ needs converting to torch, ie tree maps\n","        wandb.log({'tr/step':step, **metrix})"]}],"metadata":{"kernelspec":{"display_name":"lumi","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"04d46e33c49723ad6d04f6171fb8190218b4b7e594900f74f62bd1b58ae83499"}},"deepnote_notebook_id":"4d4ba6ac-71ff-46a1-b2e4-ae4bc91d967a"},"nbformat":4,"nbformat_minor":2}